#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <time.h>
#include <omp.h>
#include <mpi.h>

#include "initialise_header.h"

// ------------------------------------------------------------
// Function 1: Write PGM image in parallel using MPI I/O
// ------------------------------------------------------------
void write_pgm_image_parallel(unsigned char* image,
                              int xwidth, int ywidth,
                              int rows_per_proc,
                              int rank, int n_procs,
                              int maxval,
                              const char* filename)
{
    MPI_File file;
    MPI_Status status;
    int rc;

    // Open the file collectively
    rc = MPI_File_open(MPI_COMM_WORLD, filename,
                       MPI_MODE_CREATE | MPI_MODE_WRONLY,
                       MPI_INFO_NULL, &file);
    if (rc != MPI_SUCCESS) {
        fprintf(stderr, "Rank %d: Error opening file %s\n", rank, filename);
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    //  Write the PGM header (only by rank 0)
    int header_len = 0;
    if (rank == 0) {
        char header[128];
        header_len = snprintf(header, sizeof(header),
                              "P5\n# Generated by MPI+OpenMP Game of Life\n%d %d\n%d\n",
                              xwidth, ywidth, maxval);

        rc = MPI_File_write_at(file, 0, header, header_len, MPI_CHAR, &status);
        if (rc != MPI_SUCCESS) {
            fprintf(stderr, "Rank 0: Failed to write header\n");
            MPI_Abort(MPI_COMM_WORLD, 1);
        }
    }

    //  Broadcast header length to all ranks
    MPI_Bcast(&header_len, 1, MPI_INT, 0, MPI_COMM_WORLD);

    // Compute local byte count
    int local_bytes = xwidth * rows_per_proc;

    //  Compute offset using MPI_Exscan (prefix sum of byte counts)
    int bytes_before_me = 0;
    MPI_Exscan(&local_bytes, &bytes_before_me, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);
    if (rank == 0) bytes_before_me = 0;  // rank 0 has no previous bytes

    MPI_Offset my_offset = header_len + (MPI_Offset)bytes_before_me;

    //  Convert image 0/1 → 0/255 for grayscale
    for (int i = 0; i < local_bytes; i++)
        image[i] *= 255;

    MPI_Barrier(MPI_COMM_WORLD);

    //  Each rank writes its block of image data at its correct offset
    rc = MPI_File_write_at(file, my_offset, image,
                           local_bytes, MPI_UNSIGNED_CHAR, &status);
    if (rc != MPI_SUCCESS) {
        fprintf(stderr, "Rank %d: Error writing data\n", rank);
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    //  Close file collectively
    MPI_File_close(&file);
}

// ------------------------------------------------------------
// Function 2: Initialize the playground in parallel
// ------------------------------------------------------------
void initialise_playground(int k, int maxval, char* fname, int argc, char **argv)
{
    int xwidth = k, ywidth = k;
    int rank, n_procs, mpi_provided;

    printf("Starting MPI + OpenMP initialization...\n");

    //  Initialize MPI with thread support
    if (MPI_Init_thread(&argc, &argv, MPI_THREAD_FUNNELED, &mpi_provided) != MPI_SUCCESS) {
        fprintf(stderr, "Error initializing MPI\n");
        MPI_Abort(MPI_COMM_WORLD, 1);
    }
    if (mpi_provided < MPI_THREAD_FUNNELED) {
        fprintf(stderr, "MPI does not provide required thread level\n");
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    //  Get MPI communicator info
    MPI_Comm_size(MPI_COMM_WORLD, &n_procs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    //  Divide rows among processes (uneven distribution handled)
    int rows_per_proc = ywidth / n_procs;
    if (rank < ywidth % n_procs)
        rows_per_proc++;

    int cells_per_proc = rows_per_proc * xwidth;

    //  Allocate memory for local portion of the grid
    unsigned char* image = (unsigned char*)malloc(cells_per_proc * sizeof(unsigned char));
    if (!image) {
        fprintf(stderr, "Rank %d: Memory allocation failed\n", rank);
        MPI_Abort(MPI_COMM_WORLD, 1);
    }

    //  Create random playground in parallel using OpenMP
    #pragma omp parallel
    {
        unsigned int seed = (unsigned int)(time(NULL) + rank * 1000 + omp_get_thread_num() * 37);

        #pragma omp for schedule(static)
        for (int i = 0; i < cells_per_proc; i++) {
            image[i] = (unsigned char)(rand_r(&seed) % 2);  // 0 or 1
        }
    }

    if (rank == 0)
        printf("Initialized %d MPI processes × %d OpenMP threads each\n",
               n_procs, omp_get_max_threads());

    MPI_Barrier(MPI_COMM_WORLD);

    //  Write the grid to file using MPI parallel I/O
    write_pgm_image_parallel(image, xwidth, ywidth, rows_per_proc,
                             rank, n_procs, maxval, fname);

    // Free memory and finalize MPI
    free(image);
    MPI_Finalize();

    if (rank == 0)
        printf("✅ Playground initialization complete: %s\n", fname);
}
